{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBOo9qokW2Bm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pG1472ZlRaP"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "%matplotlib inline"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHR0gIgOoevD",
        "outputId": "281e5ba9-5c3d-4034-9674-e44c6272fbf8"
      },
      "source": [
        "seq_count = 1000\n",
        "seq_len = 20\n",
        "X = np.random.randint(10, size=(seq_count, seq_len), dtype=int)\n",
        "y = np.zeros((seq_count, seq_len), dtype=int)\n",
        "X[:10]"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 6, 1, 0, 3, 5, 0, 1, 9, 4, 4, 9, 8, 1, 7, 3, 3, 4, 2],\n",
              "       [3, 7, 5, 3, 2, 4, 6, 6, 5, 4, 5, 5, 3, 5, 3, 7, 1, 0, 6, 5],\n",
              "       [3, 7, 9, 8, 6, 4, 7, 3, 0, 8, 7, 8, 1, 1, 9, 3, 4, 4, 4, 6],\n",
              "       [5, 8, 0, 1, 9, 8, 6, 9, 8, 1, 5, 2, 3, 5, 8, 6, 4, 7, 0, 8],\n",
              "       [2, 3, 5, 5, 8, 6, 2, 3, 0, 3, 5, 2, 2, 4, 3, 9, 5, 6, 1, 5],\n",
              "       [3, 6, 7, 3, 8, 6, 1, 3, 3, 0, 9, 9, 6, 8, 0, 6, 8, 6, 1, 4],\n",
              "       [5, 3, 7, 0, 1, 0, 4, 0, 3, 9, 8, 2, 6, 3, 9, 6, 1, 0, 6, 7],\n",
              "       [3, 7, 2, 1, 4, 5, 3, 7, 4, 9, 5, 4, 7, 8, 9, 0, 4, 8, 0, 4],\n",
              "       [8, 9, 7, 5, 1, 9, 3, 4, 1, 1, 9, 2, 4, 2, 8, 5, 5, 5, 4, 2],\n",
              "       [9, 2, 9, 7, 0, 8, 8, 2, 6, 6, 5, 7, 7, 0, 5, 7, 4, 3, 3, 4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKa72HTgtEIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130f3dff-f1ab-4737-903b-32bb513a7951"
      },
      "source": [
        "for i in range(seq_count):\n",
        "  y[i][0] = X[i][0]\n",
        "  for j in range(1, seq_len):\n",
        "    num = X[i][j] + X[i][0]\n",
        "    y[i][j] = num - 10 if num >= 10 else num\n",
        "\n",
        "X[0:1], y[:1]"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 1, 6, 1, 0, 3, 5, 0, 1, 9, 4, 4, 9, 8, 1, 7, 3, 3, 4, 2]]),\n",
              " array([[0, 1, 6, 1, 0, 3, 5, 0, 1, 9, 4, 4, 9, 8, 1, 7, 3, 3, 4, 2]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9xGpGYXrEj6",
        "outputId": "a27da332-eda3-4a0f-f759-0b2e3a1a0e83"
      },
      "source": [
        "y = y[:,-1]\n",
        "y.shape"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNgqx_9j80pB"
      },
      "source": [
        "X, y = torch.from_numpy(X), torch.from_numpy(y)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-fm7bI188-t"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X, y)\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j6P9KFErIKA"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, network_type, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = network_type(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "      embedding = self.embed(inp)\n",
        "      out, _ = self.rnn(embedding)\n",
        "      return self.linear(out)\n"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr4sQlGu9im1"
      },
      "source": [
        "def train_model(model, loader, loss_fn, optimizer, epochs=10):\n",
        "  train_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model.forward(X_batch)  \n",
        "        y_pred = y_pred.view(-1, 200)\n",
        "        y_batch = y_batch\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(loader)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch: {epoch}, loss: {train_loss:.3f}')\n"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jd-4XwYBXk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6595dc-07ee-48e9-8490-492cd96ce881"
      },
      "source": [
        "vocab_size = 10\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "\n",
        "model = Network(torch.nn.RNN, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.001)\n",
        "train_model(model, dataset_loader, loss_fn, optimizer, 100)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 5.166\n",
            "Epoch: 1, loss: 4.181\n",
            "Epoch: 2, loss: 3.143\n",
            "Epoch: 3, loss: 2.592\n",
            "Epoch: 4, loss: 2.397\n",
            "Epoch: 5, loss: 2.330\n",
            "Epoch: 6, loss: 2.305\n",
            "Epoch: 7, loss: 2.293\n",
            "Epoch: 8, loss: 2.287\n",
            "Epoch: 9, loss: 2.285\n",
            "Epoch: 10, loss: 2.282\n",
            "Epoch: 11, loss: 2.280\n",
            "Epoch: 12, loss: 2.277\n",
            "Epoch: 13, loss: 2.276\n",
            "Epoch: 14, loss: 2.275\n",
            "Epoch: 15, loss: 2.274\n",
            "Epoch: 16, loss: 2.273\n",
            "Epoch: 17, loss: 2.272\n",
            "Epoch: 18, loss: 2.272\n",
            "Epoch: 19, loss: 2.272\n",
            "Epoch: 20, loss: 2.270\n",
            "Epoch: 21, loss: 2.271\n",
            "Epoch: 22, loss: 2.270\n",
            "Epoch: 23, loss: 2.269\n",
            "Epoch: 24, loss: 2.270\n",
            "Epoch: 25, loss: 2.270\n",
            "Epoch: 26, loss: 2.271\n",
            "Epoch: 27, loss: 2.268\n",
            "Epoch: 28, loss: 2.270\n",
            "Epoch: 29, loss: 2.268\n",
            "Epoch: 30, loss: 2.268\n",
            "Epoch: 31, loss: 2.269\n",
            "Epoch: 32, loss: 2.268\n",
            "Epoch: 33, loss: 2.268\n",
            "Epoch: 34, loss: 2.268\n",
            "Epoch: 35, loss: 2.270\n",
            "Epoch: 36, loss: 2.267\n",
            "Epoch: 37, loss: 2.268\n",
            "Epoch: 38, loss: 2.267\n",
            "Epoch: 39, loss: 2.268\n",
            "Epoch: 40, loss: 2.268\n",
            "Epoch: 41, loss: 2.268\n",
            "Epoch: 42, loss: 2.269\n",
            "Epoch: 43, loss: 2.268\n",
            "Epoch: 44, loss: 2.269\n",
            "Epoch: 45, loss: 2.267\n",
            "Epoch: 46, loss: 2.267\n",
            "Epoch: 47, loss: 2.268\n",
            "Epoch: 48, loss: 2.266\n",
            "Epoch: 49, loss: 2.268\n",
            "Epoch: 50, loss: 2.267\n",
            "Epoch: 51, loss: 2.268\n",
            "Epoch: 52, loss: 2.266\n",
            "Epoch: 53, loss: 2.267\n",
            "Epoch: 54, loss: 2.265\n",
            "Epoch: 55, loss: 2.267\n",
            "Epoch: 56, loss: 2.267\n",
            "Epoch: 57, loss: 2.268\n",
            "Epoch: 58, loss: 2.267\n",
            "Epoch: 59, loss: 2.266\n",
            "Epoch: 60, loss: 2.268\n",
            "Epoch: 61, loss: 2.266\n",
            "Epoch: 62, loss: 2.269\n",
            "Epoch: 63, loss: 2.268\n",
            "Epoch: 64, loss: 2.267\n",
            "Epoch: 65, loss: 2.269\n",
            "Epoch: 66, loss: 2.266\n",
            "Epoch: 67, loss: 2.268\n",
            "Epoch: 68, loss: 2.267\n",
            "Epoch: 69, loss: 2.268\n",
            "Epoch: 70, loss: 2.264\n",
            "Epoch: 71, loss: 2.266\n",
            "Epoch: 72, loss: 2.266\n",
            "Epoch: 73, loss: 2.266\n",
            "Epoch: 74, loss: 2.267\n",
            "Epoch: 75, loss: 2.266\n",
            "Epoch: 76, loss: 2.267\n",
            "Epoch: 77, loss: 2.268\n",
            "Epoch: 78, loss: 2.266\n",
            "Epoch: 79, loss: 2.266\n",
            "Epoch: 80, loss: 2.267\n",
            "Epoch: 81, loss: 2.267\n",
            "Epoch: 82, loss: 2.266\n",
            "Epoch: 83, loss: 2.265\n",
            "Epoch: 84, loss: 2.267\n",
            "Epoch: 85, loss: 2.265\n",
            "Epoch: 86, loss: 2.267\n",
            "Epoch: 87, loss: 2.266\n",
            "Epoch: 88, loss: 2.266\n",
            "Epoch: 89, loss: 2.266\n",
            "Epoch: 90, loss: 2.266\n",
            "Epoch: 91, loss: 2.266\n",
            "Epoch: 92, loss: 2.265\n",
            "Epoch: 93, loss: 2.265\n",
            "Epoch: 94, loss: 2.266\n",
            "Epoch: 95, loss: 2.266\n",
            "Epoch: 96, loss: 2.266\n",
            "Epoch: 97, loss: 2.266\n",
            "Epoch: 98, loss: 2.265\n",
            "Epoch: 99, loss: 2.266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPDySB1ZCdkX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne4reH1JLI5e",
        "outputId": "9af72519-8191-41bc-95c7-5e067ef36e29"
      },
      "source": [
        "model = Network(torch.nn.GRU, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(list(model.parameters()), lr=0.01)\n",
        "train_model(model, dataset_loader, loss_fn, optimizer, 100)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 5.318\n",
            "Epoch: 1, loss: 5.308\n",
            "Epoch: 2, loss: 5.298\n",
            "Epoch: 3, loss: 5.288\n",
            "Epoch: 4, loss: 5.278\n",
            "Epoch: 5, loss: 5.268\n",
            "Epoch: 6, loss: 5.258\n",
            "Epoch: 7, loss: 5.247\n",
            "Epoch: 8, loss: 5.236\n",
            "Epoch: 9, loss: 5.225\n",
            "Epoch: 10, loss: 5.212\n",
            "Epoch: 11, loss: 5.199\n",
            "Epoch: 12, loss: 5.185\n",
            "Epoch: 13, loss: 5.170\n",
            "Epoch: 14, loss: 5.154\n",
            "Epoch: 15, loss: 5.136\n",
            "Epoch: 16, loss: 5.117\n",
            "Epoch: 17, loss: 5.095\n",
            "Epoch: 18, loss: 5.072\n",
            "Epoch: 19, loss: 5.046\n",
            "Epoch: 20, loss: 5.017\n",
            "Epoch: 21, loss: 4.984\n",
            "Epoch: 22, loss: 4.948\n",
            "Epoch: 23, loss: 4.908\n",
            "Epoch: 24, loss: 4.863\n",
            "Epoch: 25, loss: 4.812\n",
            "Epoch: 26, loss: 4.755\n",
            "Epoch: 27, loss: 4.691\n",
            "Epoch: 28, loss: 4.619\n",
            "Epoch: 29, loss: 4.539\n",
            "Epoch: 30, loss: 4.450\n",
            "Epoch: 31, loss: 4.352\n",
            "Epoch: 32, loss: 4.246\n",
            "Epoch: 33, loss: 4.132\n",
            "Epoch: 34, loss: 4.012\n",
            "Epoch: 35, loss: 3.889\n",
            "Epoch: 36, loss: 3.765\n",
            "Epoch: 37, loss: 3.643\n",
            "Epoch: 38, loss: 3.526\n",
            "Epoch: 39, loss: 3.417\n",
            "Epoch: 40, loss: 3.317\n",
            "Epoch: 41, loss: 3.227\n",
            "Epoch: 42, loss: 3.147\n",
            "Epoch: 43, loss: 3.076\n",
            "Epoch: 44, loss: 3.013\n",
            "Epoch: 45, loss: 2.958\n",
            "Epoch: 46, loss: 2.909\n",
            "Epoch: 47, loss: 2.866\n",
            "Epoch: 48, loss: 2.828\n",
            "Epoch: 49, loss: 2.794\n",
            "Epoch: 50, loss: 2.764\n",
            "Epoch: 51, loss: 2.737\n",
            "Epoch: 52, loss: 2.712\n",
            "Epoch: 53, loss: 2.690\n",
            "Epoch: 54, loss: 2.670\n",
            "Epoch: 55, loss: 2.652\n",
            "Epoch: 56, loss: 2.635\n",
            "Epoch: 57, loss: 2.619\n",
            "Epoch: 58, loss: 2.605\n",
            "Epoch: 59, loss: 2.592\n",
            "Epoch: 60, loss: 2.580\n",
            "Epoch: 61, loss: 2.568\n",
            "Epoch: 62, loss: 2.558\n",
            "Epoch: 63, loss: 2.548\n",
            "Epoch: 64, loss: 2.538\n",
            "Epoch: 65, loss: 2.530\n",
            "Epoch: 66, loss: 2.521\n",
            "Epoch: 67, loss: 2.514\n",
            "Epoch: 68, loss: 2.507\n",
            "Epoch: 69, loss: 2.500\n",
            "Epoch: 70, loss: 2.493\n",
            "Epoch: 71, loss: 2.487\n",
            "Epoch: 72, loss: 2.481\n",
            "Epoch: 73, loss: 2.475\n",
            "Epoch: 74, loss: 2.470\n",
            "Epoch: 75, loss: 2.465\n",
            "Epoch: 76, loss: 2.460\n",
            "Epoch: 77, loss: 2.456\n",
            "Epoch: 78, loss: 2.451\n",
            "Epoch: 79, loss: 2.447\n",
            "Epoch: 80, loss: 2.443\n",
            "Epoch: 81, loss: 2.439\n",
            "Epoch: 82, loss: 2.435\n",
            "Epoch: 83, loss: 2.431\n",
            "Epoch: 84, loss: 2.428\n",
            "Epoch: 85, loss: 2.425\n",
            "Epoch: 86, loss: 2.422\n",
            "Epoch: 87, loss: 2.418\n",
            "Epoch: 88, loss: 2.415\n",
            "Epoch: 89, loss: 2.412\n",
            "Epoch: 90, loss: 2.410\n",
            "Epoch: 91, loss: 2.407\n",
            "Epoch: 92, loss: 2.405\n",
            "Epoch: 93, loss: 2.402\n",
            "Epoch: 94, loss: 2.400\n",
            "Epoch: 95, loss: 2.397\n",
            "Epoch: 96, loss: 2.395\n",
            "Epoch: 97, loss: 2.393\n",
            "Epoch: 98, loss: 2.390\n",
            "Epoch: 99, loss: 2.388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV13H-RHLi7s",
        "outputId": "3f128ac6-1ced-4e92-ebc3-5d64cf85c1e8"
      },
      "source": [
        "model = Network(torch.nn.LSTM, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(list(model.parameters()), lr=0.5)\n",
        "train_model(model, dataset_loader, loss_fn, optimizer, 100)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 5.209\n",
            "Epoch: 1, loss: 3.874\n",
            "Epoch: 2, loss: 2.514\n",
            "Epoch: 3, loss: 2.390\n",
            "Epoch: 4, loss: 2.349\n",
            "Epoch: 5, loss: 2.327\n",
            "Epoch: 6, loss: 2.317\n",
            "Epoch: 7, loss: 2.307\n",
            "Epoch: 8, loss: 2.301\n",
            "Epoch: 9, loss: 2.296\n",
            "Epoch: 10, loss: 2.294\n",
            "Epoch: 11, loss: 2.293\n",
            "Epoch: 12, loss: 2.297\n",
            "Epoch: 13, loss: 2.291\n",
            "Epoch: 14, loss: 2.289\n",
            "Epoch: 15, loss: 2.287\n",
            "Epoch: 16, loss: 2.286\n",
            "Epoch: 17, loss: 2.288\n",
            "Epoch: 18, loss: 2.287\n",
            "Epoch: 19, loss: 2.285\n",
            "Epoch: 20, loss: 2.281\n",
            "Epoch: 21, loss: 2.280\n",
            "Epoch: 22, loss: 2.283\n",
            "Epoch: 23, loss: 2.280\n",
            "Epoch: 24, loss: 2.280\n",
            "Epoch: 25, loss: 2.282\n",
            "Epoch: 26, loss: 2.281\n",
            "Epoch: 27, loss: 2.281\n",
            "Epoch: 28, loss: 2.278\n",
            "Epoch: 29, loss: 2.282\n",
            "Epoch: 30, loss: 2.283\n",
            "Epoch: 31, loss: 2.280\n",
            "Epoch: 32, loss: 2.281\n",
            "Epoch: 33, loss: 2.278\n",
            "Epoch: 34, loss: 2.282\n",
            "Epoch: 35, loss: 2.277\n",
            "Epoch: 36, loss: 2.281\n",
            "Epoch: 37, loss: 2.276\n",
            "Epoch: 38, loss: 2.281\n",
            "Epoch: 39, loss: 2.281\n",
            "Epoch: 40, loss: 2.283\n",
            "Epoch: 41, loss: 2.277\n",
            "Epoch: 42, loss: 2.275\n",
            "Epoch: 43, loss: 2.280\n",
            "Epoch: 44, loss: 2.278\n",
            "Epoch: 45, loss: 2.277\n",
            "Epoch: 46, loss: 2.282\n",
            "Epoch: 47, loss: 2.279\n",
            "Epoch: 48, loss: 2.283\n",
            "Epoch: 49, loss: 2.281\n",
            "Epoch: 50, loss: 2.279\n",
            "Epoch: 51, loss: 2.279\n",
            "Epoch: 52, loss: 2.278\n",
            "Epoch: 53, loss: 2.279\n",
            "Epoch: 54, loss: 2.274\n",
            "Epoch: 55, loss: 2.279\n",
            "Epoch: 56, loss: 2.278\n",
            "Epoch: 57, loss: 2.275\n",
            "Epoch: 58, loss: 2.275\n",
            "Epoch: 59, loss: 2.279\n",
            "Epoch: 60, loss: 2.276\n",
            "Epoch: 61, loss: 2.277\n",
            "Epoch: 62, loss: 2.279\n",
            "Epoch: 63, loss: 2.277\n",
            "Epoch: 64, loss: 2.277\n",
            "Epoch: 65, loss: 2.280\n",
            "Epoch: 66, loss: 2.278\n",
            "Epoch: 67, loss: 2.278\n",
            "Epoch: 68, loss: 2.275\n",
            "Epoch: 69, loss: 2.277\n",
            "Epoch: 70, loss: 2.279\n",
            "Epoch: 71, loss: 2.278\n",
            "Epoch: 72, loss: 2.277\n",
            "Epoch: 73, loss: 2.278\n",
            "Epoch: 74, loss: 2.277\n",
            "Epoch: 75, loss: 2.278\n",
            "Epoch: 76, loss: 2.280\n",
            "Epoch: 77, loss: 2.274\n",
            "Epoch: 78, loss: 2.277\n",
            "Epoch: 79, loss: 2.275\n",
            "Epoch: 80, loss: 2.274\n",
            "Epoch: 81, loss: 2.277\n",
            "Epoch: 82, loss: 2.279\n",
            "Epoch: 83, loss: 2.280\n",
            "Epoch: 84, loss: 2.279\n",
            "Epoch: 85, loss: 2.274\n",
            "Epoch: 86, loss: 2.276\n",
            "Epoch: 87, loss: 2.274\n",
            "Epoch: 88, loss: 2.276\n",
            "Epoch: 89, loss: 2.277\n",
            "Epoch: 90, loss: 2.281\n",
            "Epoch: 91, loss: 2.275\n",
            "Epoch: 92, loss: 2.274\n",
            "Epoch: 93, loss: 2.278\n",
            "Epoch: 94, loss: 2.279\n",
            "Epoch: 95, loss: 2.273\n",
            "Epoch: 96, loss: 2.278\n",
            "Epoch: 97, loss: 2.275\n",
            "Epoch: 98, loss: 2.277\n",
            "Epoch: 99, loss: 2.276\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}