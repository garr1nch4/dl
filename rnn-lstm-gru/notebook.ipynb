{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBOo9qokW2Bm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pG1472ZlRaP"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "%matplotlib inline"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHR0gIgOoevD",
        "outputId": "3e9743e7-c595-4720-ad9a-d3b90932326b"
      },
      "source": [
        "seq_count = 1000\n",
        "seq_len = 20\n",
        "X = np.random.randint(10, size=(seq_count, seq_len), dtype=int)\n",
        "y = np.zeros((seq_count, seq_len), dtype=int)\n",
        "X[:10]"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6, 5, 6, 9, 7, 1, 0, 3, 5, 8, 5, 5, 9, 4, 5, 6, 4, 0, 1, 1],\n",
              "       [9, 9, 0, 0, 9, 0, 0, 2, 3, 0, 5, 9, 9, 7, 8, 8, 9, 5, 7, 1],\n",
              "       [8, 7, 2, 2, 5, 2, 6, 0, 1, 4, 0, 4, 7, 7, 3, 3, 1, 2, 3, 7],\n",
              "       [2, 4, 1, 9, 4, 3, 7, 2, 4, 2, 5, 7, 6, 2, 6, 7, 8, 2, 2, 1],\n",
              "       [0, 7, 9, 3, 9, 9, 6, 8, 9, 3, 4, 2, 7, 1, 7, 8, 7, 5, 1, 3],\n",
              "       [3, 4, 1, 0, 2, 7, 7, 3, 7, 7, 8, 3, 7, 2, 0, 9, 8, 7, 3, 4],\n",
              "       [2, 1, 4, 0, 5, 5, 5, 8, 1, 0, 0, 4, 6, 7, 6, 2, 2, 4, 2, 8],\n",
              "       [9, 1, 8, 1, 3, 6, 2, 3, 1, 2, 1, 6, 4, 1, 6, 1, 9, 0, 2, 4],\n",
              "       [2, 5, 3, 5, 2, 6, 1, 1, 1, 9, 6, 8, 5, 2, 5, 5, 5, 4, 1, 2],\n",
              "       [4, 9, 3, 4, 5, 9, 8, 4, 0, 9, 5, 7, 7, 5, 2, 1, 9, 7, 7, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKa72HTgtEIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f3d255-eed6-4b1f-80f9-eb529bc98fa4"
      },
      "source": [
        "for i in range(seq_count):\n",
        "  y[i][0] = X[i][0]\n",
        "  for j in range(1, seq_len):\n",
        "    num = X[i][j] + X[i][0]\n",
        "    y[i][j] = num - 10 if num >= 10 else num\n",
        "\n",
        "X[0:1], y[:1]"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[6, 5, 6, 9, 7, 1, 0, 3, 5, 8, 5, 5, 9, 4, 5, 6, 4, 0, 1, 1]]),\n",
              " array([[6, 1, 2, 5, 3, 7, 6, 9, 1, 4, 1, 1, 5, 0, 1, 2, 0, 6, 7, 7]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9xGpGYXrEj6",
        "outputId": "d4259809-a5a9-4eb2-a0e1-60db8f802eef"
      },
      "source": [
        "y = y[:,-1]\n",
        "y.shape"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNgqx_9j80pB"
      },
      "source": [
        "X, y = torch.from_numpy(X), torch.from_numpy(y)"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-fm7bI188-t"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X, y)\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)"
      ],
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j6P9KFErIKA"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, network_type, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = network_type(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "      embedding = self.embed(inp)\n",
        "      _, state = self.rnn(embedding)\n",
        "      out = self.linear(state[0])\n",
        "      if isinstance(self.rnn, torch.nn.LSTM):\n",
        "        out = out.squeeze(0)\n",
        "      return out\n"
      ],
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr4sQlGu9im1"
      },
      "source": [
        "def train_model(model, loader, loss_fn, optimizer, epochs=10):\n",
        "  train_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model.forward(X_batch)  \n",
        "        y_batch = y_batch\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(loader)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch: {epoch}, loss: {train_loss:.3f}')\n"
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jd-4XwYBXk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5095f27-1684-4e18-c7df-3915adc7a653"
      },
      "source": [
        "vocab_size = 10\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "\n",
        "model = Network(torch.nn.RNN, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.Adam(list(model.parameters()), lr=0.001)\n",
        "train_model(model, dataset_loader, loss_fn1, optimizer1, 100)"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.340\n",
            "Epoch: 1, loss: 2.260\n",
            "Epoch: 2, loss: 2.227\n",
            "Epoch: 3, loss: 2.205\n",
            "Epoch: 4, loss: 2.185\n",
            "Epoch: 5, loss: 2.165\n",
            "Epoch: 6, loss: 2.145\n",
            "Epoch: 7, loss: 2.127\n",
            "Epoch: 8, loss: 2.103\n",
            "Epoch: 9, loss: 2.084\n",
            "Epoch: 10, loss: 2.059\n",
            "Epoch: 11, loss: 2.038\n",
            "Epoch: 12, loss: 2.010\n",
            "Epoch: 13, loss: 1.986\n",
            "Epoch: 14, loss: 1.954\n",
            "Epoch: 15, loss: 1.925\n",
            "Epoch: 16, loss: 1.894\n",
            "Epoch: 17, loss: 1.858\n",
            "Epoch: 18, loss: 1.820\n",
            "Epoch: 19, loss: 1.782\n",
            "Epoch: 20, loss: 1.735\n",
            "Epoch: 21, loss: 1.690\n",
            "Epoch: 22, loss: 1.639\n",
            "Epoch: 23, loss: 1.584\n",
            "Epoch: 24, loss: 1.518\n",
            "Epoch: 25, loss: 1.461\n",
            "Epoch: 26, loss: 1.392\n",
            "Epoch: 27, loss: 1.320\n",
            "Epoch: 28, loss: 1.251\n",
            "Epoch: 29, loss: 1.168\n",
            "Epoch: 30, loss: 1.097\n",
            "Epoch: 31, loss: 1.016\n",
            "Epoch: 32, loss: 0.933\n",
            "Epoch: 33, loss: 0.870\n",
            "Epoch: 34, loss: 0.792\n",
            "Epoch: 35, loss: 0.719\n",
            "Epoch: 36, loss: 0.650\n",
            "Epoch: 37, loss: 0.600\n",
            "Epoch: 38, loss: 0.541\n",
            "Epoch: 39, loss: 0.500\n",
            "Epoch: 40, loss: 0.451\n",
            "Epoch: 41, loss: 0.412\n",
            "Epoch: 42, loss: 0.360\n",
            "Epoch: 43, loss: 0.319\n",
            "Epoch: 44, loss: 0.288\n",
            "Epoch: 45, loss: 0.259\n",
            "Epoch: 46, loss: 0.239\n",
            "Epoch: 47, loss: 0.211\n",
            "Epoch: 48, loss: 0.190\n",
            "Epoch: 49, loss: 0.176\n",
            "Epoch: 50, loss: 0.164\n",
            "Epoch: 51, loss: 0.152\n",
            "Epoch: 52, loss: 0.139\n",
            "Epoch: 53, loss: 0.133\n",
            "Epoch: 54, loss: 0.120\n",
            "Epoch: 55, loss: 0.117\n",
            "Epoch: 56, loss: 0.115\n",
            "Epoch: 57, loss: 0.101\n",
            "Epoch: 58, loss: 0.090\n",
            "Epoch: 59, loss: 0.082\n",
            "Epoch: 60, loss: 0.078\n",
            "Epoch: 61, loss: 0.068\n",
            "Epoch: 62, loss: 0.061\n",
            "Epoch: 63, loss: 0.056\n",
            "Epoch: 64, loss: 0.051\n",
            "Epoch: 65, loss: 0.048\n",
            "Epoch: 66, loss: 0.045\n",
            "Epoch: 67, loss: 0.042\n",
            "Epoch: 68, loss: 0.039\n",
            "Epoch: 69, loss: 0.036\n",
            "Epoch: 70, loss: 0.034\n",
            "Epoch: 71, loss: 0.033\n",
            "Epoch: 72, loss: 0.031\n",
            "Epoch: 73, loss: 0.030\n",
            "Epoch: 74, loss: 0.028\n",
            "Epoch: 75, loss: 0.027\n",
            "Epoch: 76, loss: 0.026\n",
            "Epoch: 77, loss: 0.025\n",
            "Epoch: 78, loss: 0.024\n",
            "Epoch: 79, loss: 0.023\n",
            "Epoch: 80, loss: 0.022\n",
            "Epoch: 81, loss: 0.021\n",
            "Epoch: 82, loss: 0.021\n",
            "Epoch: 83, loss: 0.020\n",
            "Epoch: 84, loss: 0.022\n",
            "Epoch: 85, loss: 0.023\n",
            "Epoch: 86, loss: 0.047\n",
            "Epoch: 87, loss: 0.266\n",
            "Epoch: 88, loss: 0.526\n",
            "Epoch: 89, loss: 0.405\n",
            "Epoch: 90, loss: 0.282\n",
            "Epoch: 91, loss: 0.172\n",
            "Epoch: 92, loss: 0.112\n",
            "Epoch: 93, loss: 0.081\n",
            "Epoch: 94, loss: 0.059\n",
            "Epoch: 95, loss: 0.045\n",
            "Epoch: 96, loss: 0.038\n",
            "Epoch: 97, loss: 0.033\n",
            "Epoch: 98, loss: 0.029\n",
            "Epoch: 99, loss: 0.027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPDySB1ZCdkX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne4reH1JLI5e",
        "outputId": "6a0cdc4a-7243-4d4f-dac4-8e076673bb40"
      },
      "source": [
        "gru = Network(torch.nn.GRU, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn2 = torch.nn.CrossEntropyLoss()\n",
        "optimizer2 = torch.optim.Adam(list(gru.parameters()), lr=0.01)\n",
        "train_model(gru, dataset_loader, loss_fn2, optimizer2, 100)"
      ],
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.378\n",
            "Epoch: 1, loss: 2.292\n",
            "Epoch: 2, loss: 2.225\n",
            "Epoch: 3, loss: 2.115\n",
            "Epoch: 4, loss: 1.891\n",
            "Epoch: 5, loss: 1.540\n",
            "Epoch: 6, loss: 1.035\n",
            "Epoch: 7, loss: 0.560\n",
            "Epoch: 8, loss: 0.249\n",
            "Epoch: 9, loss: 0.104\n",
            "Epoch: 10, loss: 0.045\n",
            "Epoch: 11, loss: 0.021\n",
            "Epoch: 12, loss: 0.012\n",
            "Epoch: 13, loss: 0.008\n",
            "Epoch: 14, loss: 0.006\n",
            "Epoch: 15, loss: 0.005\n",
            "Epoch: 16, loss: 0.004\n",
            "Epoch: 17, loss: 0.004\n",
            "Epoch: 18, loss: 0.003\n",
            "Epoch: 19, loss: 0.003\n",
            "Epoch: 20, loss: 0.003\n",
            "Epoch: 21, loss: 0.003\n",
            "Epoch: 22, loss: 0.002\n",
            "Epoch: 23, loss: 0.002\n",
            "Epoch: 24, loss: 0.002\n",
            "Epoch: 25, loss: 0.002\n",
            "Epoch: 26, loss: 0.002\n",
            "Epoch: 27, loss: 0.002\n",
            "Epoch: 28, loss: 0.002\n",
            "Epoch: 29, loss: 0.002\n",
            "Epoch: 30, loss: 0.001\n",
            "Epoch: 31, loss: 0.001\n",
            "Epoch: 32, loss: 0.001\n",
            "Epoch: 33, loss: 0.001\n",
            "Epoch: 34, loss: 0.001\n",
            "Epoch: 35, loss: 0.001\n",
            "Epoch: 36, loss: 0.001\n",
            "Epoch: 37, loss: 0.001\n",
            "Epoch: 38, loss: 0.001\n",
            "Epoch: 39, loss: 0.001\n",
            "Epoch: 40, loss: 0.001\n",
            "Epoch: 41, loss: 0.001\n",
            "Epoch: 42, loss: 0.001\n",
            "Epoch: 43, loss: 0.001\n",
            "Epoch: 44, loss: 0.001\n",
            "Epoch: 45, loss: 0.001\n",
            "Epoch: 46, loss: 0.001\n",
            "Epoch: 47, loss: 0.001\n",
            "Epoch: 48, loss: 0.001\n",
            "Epoch: 49, loss: 0.001\n",
            "Epoch: 50, loss: 0.001\n",
            "Epoch: 51, loss: 0.001\n",
            "Epoch: 52, loss: 0.001\n",
            "Epoch: 53, loss: 0.001\n",
            "Epoch: 54, loss: 0.001\n",
            "Epoch: 55, loss: 0.001\n",
            "Epoch: 56, loss: 0.001\n",
            "Epoch: 57, loss: 0.001\n",
            "Epoch: 58, loss: 0.001\n",
            "Epoch: 59, loss: 0.000\n",
            "Epoch: 60, loss: 0.000\n",
            "Epoch: 61, loss: 0.000\n",
            "Epoch: 62, loss: 0.000\n",
            "Epoch: 63, loss: 0.000\n",
            "Epoch: 64, loss: 0.000\n",
            "Epoch: 65, loss: 0.000\n",
            "Epoch: 66, loss: 0.000\n",
            "Epoch: 67, loss: 0.000\n",
            "Epoch: 68, loss: 0.000\n",
            "Epoch: 69, loss: 0.000\n",
            "Epoch: 70, loss: 0.000\n",
            "Epoch: 71, loss: 0.000\n",
            "Epoch: 72, loss: 0.000\n",
            "Epoch: 73, loss: 0.000\n",
            "Epoch: 74, loss: 0.000\n",
            "Epoch: 75, loss: 0.000\n",
            "Epoch: 76, loss: 0.000\n",
            "Epoch: 77, loss: 0.000\n",
            "Epoch: 78, loss: 0.000\n",
            "Epoch: 79, loss: 0.000\n",
            "Epoch: 80, loss: 0.000\n",
            "Epoch: 81, loss: 0.000\n",
            "Epoch: 82, loss: 0.000\n",
            "Epoch: 83, loss: 0.000\n",
            "Epoch: 84, loss: 0.000\n",
            "Epoch: 85, loss: 0.000\n",
            "Epoch: 86, loss: 0.000\n",
            "Epoch: 87, loss: 0.000\n",
            "Epoch: 88, loss: 0.000\n",
            "Epoch: 89, loss: 0.000\n",
            "Epoch: 90, loss: 0.000\n",
            "Epoch: 91, loss: 0.000\n",
            "Epoch: 92, loss: 0.000\n",
            "Epoch: 93, loss: 0.000\n",
            "Epoch: 94, loss: 0.000\n",
            "Epoch: 95, loss: 0.000\n",
            "Epoch: 96, loss: 0.000\n",
            "Epoch: 97, loss: 0.000\n",
            "Epoch: 98, loss: 0.000\n",
            "Epoch: 99, loss: 0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV13H-RHLi7s",
        "outputId": "b91a4384-ef91-4008-b339-a39c487aa48f"
      },
      "source": [
        "lstm = Network(torch.nn.LSTM, vocab_size, embed_dim, hidden_dim)\n",
        "loss_fn3 = torch.nn.CrossEntropyLoss()\n",
        "optimizer3 = torch.optim.Adam(list(lstm.parameters()), lr=0.01)\n",
        "train_model(lstm, dataset_loader, loss_fn3, optimizer3, 100)"
      ],
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.336\n",
            "Epoch: 1, loss: 2.273\n",
            "Epoch: 2, loss: 2.236\n",
            "Epoch: 3, loss: 2.157\n",
            "Epoch: 4, loss: 2.001\n",
            "Epoch: 5, loss: 1.736\n",
            "Epoch: 6, loss: 1.386\n",
            "Epoch: 7, loss: 0.967\n",
            "Epoch: 8, loss: 0.582\n",
            "Epoch: 9, loss: 0.322\n",
            "Epoch: 10, loss: 0.151\n",
            "Epoch: 11, loss: 0.074\n",
            "Epoch: 12, loss: 0.037\n",
            "Epoch: 13, loss: 0.023\n",
            "Epoch: 14, loss: 0.016\n",
            "Epoch: 15, loss: 0.011\n",
            "Epoch: 16, loss: 0.009\n",
            "Epoch: 17, loss: 0.007\n",
            "Epoch: 18, loss: 0.006\n",
            "Epoch: 19, loss: 0.005\n",
            "Epoch: 20, loss: 0.005\n",
            "Epoch: 21, loss: 0.004\n",
            "Epoch: 22, loss: 0.004\n",
            "Epoch: 23, loss: 0.004\n",
            "Epoch: 24, loss: 0.003\n",
            "Epoch: 25, loss: 0.003\n",
            "Epoch: 26, loss: 0.003\n",
            "Epoch: 27, loss: 0.003\n",
            "Epoch: 28, loss: 0.002\n",
            "Epoch: 29, loss: 0.002\n",
            "Epoch: 30, loss: 0.002\n",
            "Epoch: 31, loss: 0.002\n",
            "Epoch: 32, loss: 0.002\n",
            "Epoch: 33, loss: 0.002\n",
            "Epoch: 34, loss: 0.002\n",
            "Epoch: 35, loss: 0.002\n",
            "Epoch: 36, loss: 0.002\n",
            "Epoch: 37, loss: 0.002\n",
            "Epoch: 38, loss: 0.001\n",
            "Epoch: 39, loss: 0.001\n",
            "Epoch: 40, loss: 0.001\n",
            "Epoch: 41, loss: 0.001\n",
            "Epoch: 42, loss: 0.001\n",
            "Epoch: 43, loss: 0.001\n",
            "Epoch: 44, loss: 0.001\n",
            "Epoch: 45, loss: 0.001\n",
            "Epoch: 46, loss: 0.001\n",
            "Epoch: 47, loss: 0.001\n",
            "Epoch: 48, loss: 0.001\n",
            "Epoch: 49, loss: 0.001\n",
            "Epoch: 50, loss: 0.001\n",
            "Epoch: 51, loss: 0.001\n",
            "Epoch: 52, loss: 0.001\n",
            "Epoch: 53, loss: 0.001\n",
            "Epoch: 54, loss: 0.001\n",
            "Epoch: 55, loss: 0.001\n",
            "Epoch: 56, loss: 0.001\n",
            "Epoch: 57, loss: 0.001\n",
            "Epoch: 58, loss: 0.001\n",
            "Epoch: 59, loss: 0.001\n",
            "Epoch: 60, loss: 0.001\n",
            "Epoch: 61, loss: 0.001\n",
            "Epoch: 62, loss: 0.001\n",
            "Epoch: 63, loss: 0.001\n",
            "Epoch: 64, loss: 0.001\n",
            "Epoch: 65, loss: 0.001\n",
            "Epoch: 66, loss: 0.001\n",
            "Epoch: 67, loss: 0.001\n",
            "Epoch: 68, loss: 0.001\n",
            "Epoch: 69, loss: 0.001\n",
            "Epoch: 70, loss: 0.000\n",
            "Epoch: 71, loss: 0.000\n",
            "Epoch: 72, loss: 0.000\n",
            "Epoch: 73, loss: 0.000\n",
            "Epoch: 74, loss: 0.000\n",
            "Epoch: 75, loss: 0.000\n",
            "Epoch: 76, loss: 0.000\n",
            "Epoch: 77, loss: 0.000\n",
            "Epoch: 78, loss: 0.000\n",
            "Epoch: 79, loss: 0.000\n",
            "Epoch: 80, loss: 0.000\n",
            "Epoch: 81, loss: 0.000\n",
            "Epoch: 82, loss: 0.000\n",
            "Epoch: 83, loss: 0.000\n",
            "Epoch: 84, loss: 0.000\n",
            "Epoch: 85, loss: 0.000\n",
            "Epoch: 86, loss: 0.000\n",
            "Epoch: 87, loss: 0.000\n",
            "Epoch: 88, loss: 0.000\n",
            "Epoch: 89, loss: 0.000\n",
            "Epoch: 90, loss: 0.000\n",
            "Epoch: 91, loss: 0.000\n",
            "Epoch: 92, loss: 0.000\n",
            "Epoch: 93, loss: 0.000\n",
            "Epoch: 94, loss: 0.000\n",
            "Epoch: 95, loss: 0.000\n",
            "Epoch: 96, loss: 0.000\n",
            "Epoch: 97, loss: 0.000\n",
            "Epoch: 98, loss: 0.000\n",
            "Epoch: 99, loss: 0.000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}